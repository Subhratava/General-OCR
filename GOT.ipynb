{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b9fd97-99ee-4e65-b218-2acc44f90665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d717a3-6cbe-4048-9c02-9f6d939d45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Load model & processor\n",
    "# model = AutoModelForImageTextToText.from_pretrained(\n",
    "#     \"stepfun-ai/GOT-OCR-2.0-hf\",\n",
    "#     device_map=\"auto\",  # or use \"cuda\" with .to(device)\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n",
    "\n",
    "# # Load the image from URL\n",
    "# image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n",
    "# response = requests.get(image_url)\n",
    "# image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# # Preprocess\n",
    "# inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # Generate text\n",
    "# generate_ids = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=False,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     stop_strings=\"<|im_end|>\",\n",
    "#     max_new_tokens=4096,\n",
    "# )\n",
    "\n",
    "# # Decode output\n",
    "# output_text = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "# print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6abbea37-6b0a-434d-b1f5-13f9ea64e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'R&D QUALITY IMPROVEMENT\\nSUGGESTION/SOLUTION FORM\\nName/Phone Ext. : M. Hamann, P. Harper, P. Martinez\\nDate: 9/3/92\\nSupervisor/Manager: J. S. Wigand\\nR&D Group: Licensee\\nSuggestion:\\nDiscontinue coal retention analyses on licensee submitted\\nproduct samples. (Note: Coal Retention testing is not\\nperformed by most licensees. Other B&W physical\\nmeasurements as ends stability and inspection for soft\\nspots in cigarettes are thought to be sufficient measures\\nto assure cigarette physical integrity. The proposed\\naction will increase laboratory productivity.)\\nSuggested Solution(s) : Delete coal retention from the list of standard\\nanalyses performed on licensee submitted\\nproduct samples. Special requests for coal\\nretention testing could still be submitted on\\nan exception basis.\\nHave you contacted your Manager/Supervisor?\\nYes\\nNo\\nManager Comments: Manager, please contact suggester and forward\\ncomments to the Quality Council.\\nqip.wp\\n597005708'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "print(torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n",
    "# processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"got_ocr_model\", device_map=device)\n",
    "processor = AutoProcessor.from_pretrained(\"got_ocr_model\")\n",
    "\n",
    "# image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n",
    "# inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generate_ids = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=False,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     stop_strings=\"<|im_end|>\",\n",
    "#     max_new_tokens=4096,\n",
    "# )\n",
    "\n",
    "# processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce98622-f695-474c-af73-9a4f83dd0b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save them to local folder\n",
    "# model.save_pretrained(\"got_ocr_model\")\n",
    "# processor.save_pretrained(\"got_ocr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354af3a9-2baf-4970-be56-de325aaa6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ocr(path):\n",
    "    image = path\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        stop_strings=\"<|im_end|>\",\n",
    "        max_new_tokens=4096,\n",
    "    )\n",
    "    \n",
    "    return processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e82cda4-30c9-4aa0-a68b-19e2d2bcdbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”² Drag to select screen area...\n",
      "âœ… Snip saved as 'snipped_area.png'\n",
      "ðŸ“‹ Image copied to clipboard!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'JupyterLab Python3(ipykernel)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageGrab, Image\n",
    "import pyautogui\n",
    "import io\n",
    "import win32clipboard  # Windows only\n",
    "from PIL import ImageTk\n",
    "import time\n",
    "\n",
    "coords = {\"x1\": 0, \"y1\": 0, \"x2\": 0, \"y2\": 0}\n",
    "rect_id = None\n",
    "\n",
    "def on_mouse_down(event):\n",
    "    coords[\"x1\"] = event.x\n",
    "    coords[\"y1\"] = event.y\n",
    "    global rect_id\n",
    "    rect_id = canvas.create_rectangle(event.x, event.y, event.x, event.y, outline='red', width=2)\n",
    "\n",
    "def on_mouse_move(event):\n",
    "    if rect_id:\n",
    "        canvas.coords(rect_id, coords[\"x1\"], coords[\"y1\"], event.x, event.y)\n",
    "\n",
    "def on_mouse_up(event):\n",
    "    coords[\"x2\"] = event.x\n",
    "    coords[\"y2\"] = event.y\n",
    "    root.destroy()\n",
    "\n",
    "def image_to_clipboard(img: Image.Image):\n",
    "    output = io.BytesIO()\n",
    "    img.convert(\"RGB\").save(output, \"BMP\")\n",
    "    data = output.getvalue()[14:]  # Skip BMP header\n",
    "    output.close()\n",
    "\n",
    "    win32clipboard.OpenClipboard()\n",
    "    win32clipboard.EmptyClipboard()\n",
    "    win32clipboard.SetClipboardData(win32clipboard.CF_DIB, data)\n",
    "    win32clipboard.CloseClipboard()\n",
    "\n",
    "# --- UI setup ---\n",
    "time.sleep(0.5)\n",
    "root = tk.Tk()\n",
    "root.attributes(\"-alpha\", 0.3)\n",
    "root.attributes(\"-fullscreen\", True)\n",
    "root.attributes(\"-topmost\", True)\n",
    "root.configure(bg='gray')\n",
    "\n",
    "canvas = tk.Canvas(root, cursor=\"cross\", bg='gray')\n",
    "canvas.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "canvas.bind(\"<ButtonPress-1>\", on_mouse_down)\n",
    "canvas.bind(\"<B1-Motion>\", on_mouse_move)\n",
    "canvas.bind(\"<ButtonRelease-1>\", on_mouse_up)\n",
    "\n",
    "print(\"ðŸ”² Drag to select screen area...\")\n",
    "root.mainloop()\n",
    "\n",
    "# Get coordinates\n",
    "x1, y1 = coords[\"x1\"], coords[\"y1\"]\n",
    "x2, y2 = coords[\"x2\"], coords[\"y2\"]\n",
    "x1, x2 = sorted([x1, x2])\n",
    "y1, y2 = sorted([y1, y2])\n",
    "width, height = x2 - x1, y2 - y1\n",
    "\n",
    "# Screenshot selected area\n",
    "screenshot = pyautogui.screenshot(region=(x1, y1, width, height))\n",
    "screenshot.save(\"snipped_area.png\")\n",
    "image_to_clipboard(screenshot)\n",
    "\n",
    "print(\"âœ… Snip saved as 'snipped_area.png'\")\n",
    "print(\"ðŸ“‹ Image copied to clipboard!\")\n",
    "get_ocr(\"snipped_area.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16040f-334a-4b93-8341-885c9cb92f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e9970-bc4d-4369-ac3d-fcf773a62370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e3b2e-ba5a-4705-ad7c-1b2a8f391dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efa6f0-15dd-4cdd-adf3-c90eb9dfed94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
